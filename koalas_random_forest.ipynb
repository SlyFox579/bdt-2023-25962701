{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SlyFox579/bdt-2023-25962701/blob/main/koalas_random_forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVwqmJqpUnH-"
      },
      "source": [
        "An implementation for porting to other platforms and discussion (this is not to do exploratory analysis but rather to consider the APIs and technologies involved - it is not intended to be a good or reference solution to this problem)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT2L-4tLUnID"
      },
      "source": [
        "Obtain the data from Google Cloud Storage buckets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3u8xK6WUnID",
        "outputId": "ad90f5cf-b649-4efb-b885-e2d2f8fe9198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-06 18:09:26--  https://storage.googleapis.com/bdt-spark-store/external_sources.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.139.207, 74.125.141.207, 173.194.211.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.139.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15503836 (15M) [text/csv]\n",
            "Saving to: ‘gcs_external_sources.csv’\n",
            "\n",
            "gcs_external_source 100%[===================>]  14.79M  15.6MB/s    in 0.9s    \n",
            "\n",
            "2023-11-06 18:09:27 (15.6 MB/s) - ‘gcs_external_sources.csv’ saved [15503836/15503836]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://storage.googleapis.com/bdt-spark-store/external_sources.csv -O gcs_external_sources.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Prd3S1RfUnIF",
        "outputId": "62425538-ac44-46c7-9892-4e74f1fef2e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-06 18:09:30--  https://storage.googleapis.com/bdt-spark-store/internal_data.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.139.207, 74.125.141.207, 173.194.211.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.139.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 152978396 (146M) [text/csv]\n",
            "Saving to: ‘gcs_internal_data.csv’\n",
            "\n",
            "gcs_internal_data.c 100%[===================>] 145.89M  36.4MB/s    in 4.5s    \n",
            "\n",
            "2023-11-06 18:09:35 (32.7 MB/s) - ‘gcs_internal_data.csv’ saved [152978396/152978396]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://storage.googleapis.com/bdt-spark-store/internal_data.csv -O gcs_internal_data.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get spark\n",
        "VERSION='3.5.0'\n",
        "!wget https://dlcdn.apache.org/spark/spark-$VERSION/spark-$VERSION-bin-hadoop3.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1BUQ-QLXAdy",
        "outputId": "20f5e39c-eb5b-4169-8bfa-21b2d3d551a0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-06 15:59:56--  https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400395283 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.0-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.5.0-bin-had 100%[===================>] 381.85M  22.9MB/s    in 5.3s    \n",
            "\n",
            "2023-11-06 16:00:12 (71.5 MB/s) - ‘spark-3.5.0-bin-hadoop3.tgz’ saved [400395283/400395283]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decompress spark\n",
        "!tar xf spark-$VERSION-bin-hadoop3.tgz\n",
        "\n",
        "# install python package to help with system paths\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "uaoXT7TaXQhV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let Colab know where the java and spark folders are\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/spark-{VERSION}-bin-hadoop3\""
      ],
      "metadata": {
        "id": "CEGvNe0vXUMz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lsb_release -a\n",
        "!apt-get update\n",
        "# Install java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JztQm0zdYL5-",
        "outputId": "7091a98b-3966-4857-be63-5f5d66ebfa53"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No LSB modules are available.\n",
            "Distributor ID:\tUbuntu\n",
            "Description:\tUbuntu 22.04.2 LTS\n",
            "Release:\t22.04\n",
            "Codename:\tjammy\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [46.6 kB]\n",
            "Get:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,186 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [49.8 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,279 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,013 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,455 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,419 kB]\n",
            "Hit:17 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:18 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main Sources [2,231 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main amd64 Packages [1,145 kB]\n",
            "Fetched 10.2 MB in 4s (2,434 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "Jj17s-PeXZsb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import databricks.koalas as ks\n",
        "\n",
        "# Read the CSV files using Koalas\n",
        "df_data = ks.read_csv('gcs_internal_data.csv')\n",
        "df_ext = ks.read_csv('gcs_external_sources.csv')"
      ],
      "metadata": {
        "id": "26NF36xnGPtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr51KLScUnIH"
      },
      "source": [
        "Join them on their common identifier key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qmSns2IUnII"
      },
      "outputs": [],
      "source": [
        "import databricks.koalas as ks\n",
        "\n",
        "# Assuming you have df_data and df_ext DataFrames already created.\n",
        "\n",
        "# Merge the DataFrames on the 'SK_ID_CURR' column using an inner join.\n",
        "df_full = ks.merge(df_data, df_ext, on='SK_ID_CURR', how='inner')\n",
        "\n",
        "# Display the first few rows of the merged DataFrame.\n",
        "df_full.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpDv2q27UnIJ"
      },
      "source": [
        "We will filter a few features out for the sake of this example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrT8dasOUnIK"
      },
      "outputs": [],
      "source": [
        "columns_extract = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n",
        "                  'DAYS_BIRTH', 'DAYS_EMPLOYED', 'NAME_EDUCATION_TYPE',\n",
        "                  'DAYS_ID_PUBLISH', 'CODE_GENDER', 'AMT_ANNUITY',\n",
        "                  'DAYS_REGISTRATION', 'AMT_GOODS_PRICE', 'AMT_CREDIT',\n",
        "                  'ORGANIZATION_TYPE', 'DAYS_LAST_PHONE_CHANGE',\n",
        "                  'NAME_INCOME_TYPE', 'AMT_INCOME_TOTAL', 'OWN_CAR_AGE', 'TARGET']\n",
        "df = df_full[columns_extract]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnMmjj42UnIM"
      },
      "source": [
        "Let's obtain a train and test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GS3XV46XUnIN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "ks.set_option(\"compute.default_random_state\", np.random.RandomState(101))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BppGX8y_UnIO"
      },
      "outputs": [],
      "source": [
        "# Shuffle the DataFrame with random_state for reproducibility.\n",
        "df_shuffled = df.sample(frac=1, random_state=np.random.RandomState(101))\n",
        "\n",
        "# Calculate the split point based on the desired train-test split ratio.\n",
        "split_point = int(0.8 * len(df_shuffled))\n",
        "\n",
        "# Split the DataFrame into train and test sets.\n",
        "train = df_shuffled.iloc[:split_point]\n",
        "test = df_shuffled.iloc[split_point:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6Pb7OjiUnIO"
      },
      "outputs": [],
      "source": [
        "print(train.TARGET.value_counts()/len(train.index))\n",
        "print(test.TARGET.value_counts()/len(test.index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ipXqctnUnIQ"
      },
      "source": [
        "Handle the categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hz7wD81fUnIR"
      },
      "outputs": [],
      "source": [
        "# Perform one-hot encoding on categorical columns for the 'train' and 'test' DataFrames.\n",
        "train = ks.get_dummies(train)\n",
        "test = ks.get_dummies(test)\n",
        "\n",
        "# Print the shapes of the resulting DataFrames.\n",
        "print('Training Features shape: ', train.shape)\n",
        "print('Testing Features shape: ', test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmtX19eXUnIT"
      },
      "source": [
        "Align the training and test data (as the test data may not have the same columns in the encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSJmgweKUnIT"
      },
      "outputs": [],
      "source": [
        "# Align the training and testing data, keep only columns present in both dataframes\n",
        "train, test = train.align(test, join = 'inner', axis = 1)\n",
        "\n",
        "print('Training Features shape: ', train.shape)\n",
        "print('Testing Features shape: ', test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfrrKrjYUnIU"
      },
      "source": [
        "Get labels from data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCpJ-iyAUnIV"
      },
      "outputs": [],
      "source": [
        "train_labels = train['TARGET']\n",
        "test_labels = test['TARGET']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3_N3n66UnIX"
      },
      "source": [
        "Fill in missing data and scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5utn43KiUnIY"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer as Imputer\n",
        "\n",
        "# Assuming you have 'train' and 'test' DataFrames with the 'TARGET' column and other features.\n",
        "\n",
        "# Drop the 'TARGET' column from the training and testing data\n",
        "if 'TARGET' in train:\n",
        "    train = train.drop(columns=['TARGET'])\n",
        "    test = test.drop(columns=['TARGET'])\n",
        "else:\n",
        "    train = train.copy()\n",
        "    test = test.copy()\n",
        "\n",
        "# Feature names\n",
        "features = list(train.columns)\n",
        "\n",
        "# Median imputation of missing values\n",
        "imputer = Imputer(strategy='median')\n",
        "\n",
        "# Scale each feature to 0-1\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit on the training data and transform both training and testing data\n",
        "imputer.fit(train)\n",
        "train = ks.from_pandas(imputer.transform(train))\n",
        "test = ks.from_pandas(imputer.transform(test))\n",
        "\n",
        "# Fit on the training data and transform both training and testing data\n",
        "scaler.fit(train)\n",
        "train = ks.from_pandas(scaler.transform(train))\n",
        "test = ks.from_pandas(scaler.transform(test))\n",
        "\n",
        "# Print the shapes of the resulting DataFrames\n",
        "print('Training data shape: ', train.shape)\n",
        "print('Testing data shape: ', test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Chx1hSsLUnIZ"
      },
      "source": [
        "Fit random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "6kpvAc6WUnIa"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Make the random forest classifier\n",
        "random_forest = RandomForestClassifier(n_estimators = 100,\n",
        "                                       random_state = 50,\n",
        "                                       verbose = 1, n_jobs = -1)\n",
        "# Train on the training data\n",
        "random_forest.fit(train, train_labels)\n",
        "\n",
        "# Extract feature importances\n",
        "feature_importance_values = random_forest.feature_importances_\n",
        "feature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = random_forest.predict(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBKqWT4pUnIc"
      },
      "source": [
        "Evaluate on test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntt_qcBVUnId"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "print(accuracy_score(test_labels, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQ-pFtEvUnId"
      },
      "outputs": [],
      "source": [
        "feature_importances.sort_values('importance', ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-hBxFiWUnIe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ass3",
      "language": "python",
      "name": "ass3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}