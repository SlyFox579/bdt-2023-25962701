{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SlyFox579/bdt-2023-25962701/blob/main/pyspark_random_forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX7p3ux58qHd"
      },
      "source": [
        "An implementation for porting to other platforms and discussion (this is not to do exploratory analysis but rather to consider the APIs and technologies involved - it is not intended to be a good or reference solution to this problem)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1zo8T-r8qHj"
      },
      "source": [
        "Obtain the data from Google Cloud Storage buckets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0u_sbiJn8qHk",
        "outputId": "209ef873-51b9-4458-a03a-6e7875da0b98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-06 19:04:48--  https://storage.googleapis.com/bdt-spark-store/external_sources.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.2.207, 2607:f8b0:4023:c0d::cf, 2607:f8b0:4023:c03::cf\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.2.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15503836 (15M) [text/csv]\n",
            "Saving to: ‘gcs_external_sources.csv’\n",
            "\n",
            "gcs_external_source 100%[===================>]  14.79M  10.6MB/s    in 1.4s    \n",
            "\n",
            "2023-11-06 19:04:50 (10.6 MB/s) - ‘gcs_external_sources.csv’ saved [15503836/15503836]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://storage.googleapis.com/bdt-spark-store/external_sources.csv -O gcs_external_sources.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XC4GYxLJ8qHl",
        "outputId": "5cd4abfc-0410-4e63-a184-161e9c0e30e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-06 19:04:53--  https://storage.googleapis.com/bdt-spark-store/internal_data.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.2.207, 2607:f8b0:4023:c0d::cf, 2607:f8b0:4023:c03::cf\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.2.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 152978396 (146M) [text/csv]\n",
            "Saving to: ‘gcs_internal_data.csv’\n",
            "\n",
            "gcs_internal_data.c 100%[===================>] 145.89M  25.1MB/s    in 6.5s    \n",
            "\n",
            "2023-11-06 19:05:01 (22.3 MB/s) - ‘gcs_internal_data.csv’ saved [152978396/152978396]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://storage.googleapis.com/bdt-spark-store/internal_data.csv -O gcs_internal_data.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lsb_release -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0lLtc-CDQMK",
        "outputId": "5b930d8d-e419-4b23-f0b1-de592ff74643"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No LSB modules are available.\n",
            "Distributor ID:\tUbuntu\n",
            "Description:\tUbuntu 22.04.2 LTS\n",
            "Release:\t22.04\n",
            "Codename:\tjammy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeewXAj4DVRI",
        "outputId": "3c032f92-b309-4735-a206-f81fee7d83bb"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com] [1 InRelease 0 B/3,626 B 0%] [Connectin\r0% [Waiting for headers] [Connecting to security.ubuntu.com] [Connecting to ppa.launchpadcontent.net\r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com] [Connecting to ppa.launchpadcontent.net\r                                                                                                    \rHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,455 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,279 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,013 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,186 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 5,274 kB in 3s (2,047 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "zIoahLziDX-4"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get spark\n",
        "VERSION='3.5.0'\n",
        "!wget https://dlcdn.apache.org/spark/spark-$VERSION/spark-$VERSION-bin-hadoop3.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk5c3HcpDbBM",
        "outputId": "50bd8870-ecf9-4ac5-e87e-4b07881b55df"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-06 19:05:24--  https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400395283 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.0-bin-hadoop3.tgz.1’\n",
            "\n",
            "spark-3.5.0-bin-had 100%[===================>] 381.85M   198MB/s    in 1.9s    \n",
            "\n",
            "2023-11-06 19:05:26 (198 MB/s) - ‘spark-3.5.0-bin-hadoop3.tgz.1’ saved [400395283/400395283]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decompress spark\n",
        "!tar xf spark-$VERSION-bin-hadoop3.tgz\n",
        "\n",
        "# install python package to help with system paths\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "LYWNg69iENlz"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let Colab know where the java and spark folders are\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/spark-{VERSION}-bin-hadoop3\""
      ],
      "metadata": {
        "id": "cQf-Y6JAEOYH"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "esDLf6O2EWPj"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaLKGakk8qHm"
      },
      "source": [
        "Read in data sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "qdkbK8nV8qHn"
      },
      "outputs": [],
      "source": [
        "# Read a CSV file and create a DataFrame\n",
        "df_data = spark.read.csv(\"gcs_internal_data.csv\", header=True, inferSchema=True)\n",
        "df_ext = spark.read.csv(\"gcs_external_sources.csv\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgCQficm8qHo"
      },
      "source": [
        "Join them on their common identifier key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "sZwuI3-g8qHr"
      },
      "outputs": [],
      "source": [
        "# Perform an inner join on the 'SK_ID_CURR' column\n",
        "df_full = df_data.join(df_ext, on='SK_ID_CURR', how='inner')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3EvFKoP8qHr"
      },
      "source": [
        "We will filter a few features out for the sake of this example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "ZTq4nfU78qHt"
      },
      "outputs": [],
      "source": [
        "# List of columns to extract\n",
        "columns_extract = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n",
        "                  'DAYS_BIRTH', 'DAYS_EMPLOYED', 'NAME_EDUCATION_TYPE',\n",
        "                  'DAYS_ID_PUBLISH', 'CODE_GENDER', 'AMT_ANNUITY',\n",
        "                  'DAYS_REGISTRATION', 'AMT_GOODS_PRICE', 'AMT_CREDIT',\n",
        "                  'ORGANIZATION_TYPE', 'DAYS_LAST_PHONE_CHANGE',\n",
        "                  'NAME_INCOME_TYPE', 'AMT_INCOME_TOTAL', 'OWN_CAR_AGE', 'TARGET']\n",
        "\n",
        "# Select the specified columns from 'df_full'\n",
        "df = df_full.select(columns_extract)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5aKL5zd8qHu"
      },
      "source": [
        "Let's obtain a train and test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "BqvG_uhK8qHv"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Set a specific seed value (e.g., 101)\n",
        "seed_value = 101\n",
        "\n",
        "# Create a random number generator with the specified seed\n",
        "rand_gen = random.Random(seed_value)\n",
        "\n",
        "# Use the rand_gen to generate random numbers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rand\n",
        "\n",
        "# Randomly shuffle the DataFrame\n",
        "df = df.orderBy(rand())\n",
        "\n",
        "# Split the DataFrame into training and test sets\n",
        "split_ratio = 0.8  # 80% for training, 20% for testing\n",
        "split_seed = 42     # Set a specific seed for reproducibility\n",
        "\n",
        "train, test = df.randomSplit([split_ratio, 1 - split_ratio], seed=split_seed)\n"
      ],
      "metadata": {
        "id": "u5m-4nW6QGQD"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzExDKPy8qHw",
        "outputId": "30f1e1de-f48a-4725-fbfb-e5f6bbc3787b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data:\n",
            "+------+------+-------------------+\n",
            "|TARGET| count| Relative_Frequency|\n",
            "+------+------+-------------------+\n",
            "|     1| 19905|0.08092351599565806|\n",
            "|     0|226055|  0.919023632675131|\n",
            "+------+------+-------------------+\n",
            "\n",
            "Test Data:\n",
            "+------+-----+-------------------+\n",
            "|TARGET|count| Relative_Frequency|\n",
            "+------+-----+-------------------+\n",
            "|     1| 4887|0.07940531318547404|\n",
            "|     0|56661| 0.9206434316353888|\n",
            "+------+-----+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, count\n",
        "\n",
        "# Calculate value counts and relative frequencies for 'TARGET' column in train\n",
        "train_value_counts = train.groupBy('TARGET').agg(count('*').alias('count'))\n",
        "train_relative_frequencies = train_value_counts.withColumn(\n",
        "    'Relative_Frequency', col('count') / train.count()\n",
        ")\n",
        "\n",
        "# Calculate value counts and relative frequencies for 'TARGET' column in test\n",
        "test_value_counts = test.groupBy('TARGET').agg(count('*').alias('count'))\n",
        "test_relative_frequencies = test_value_counts.withColumn(\n",
        "    'Relative_Frequency', col('count') / test.count()\n",
        ")\n",
        "\n",
        "# Show the results\n",
        "print(\"Train Data:\")\n",
        "train_relative_frequencies.show()\n",
        "\n",
        "print(\"Test Data:\")\n",
        "test_relative_frequencies.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAu3uKJJ8qHz"
      },
      "source": [
        "Handle the categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSVlSu2l8qH0",
        "outputId": "461a040e-fcb8-40dd-9322-542f4b5e8a9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Features shape: (245961, 26)\n",
            "Testing Features shape: (61554, 26)\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# List of categorical column names to one-hot encode\n",
        "categorical_columns = ['NAME_EDUCATION_TYPE', 'CODE_GENDER', 'ORGANIZATION_TYPE', 'NAME_INCOME_TYPE']  # Replace with your column names\n",
        "\n",
        "# Create an empty list to store one-hot encoded column names\n",
        "one_hot_encoded_cols = []\n",
        "\n",
        "# Create a Pipeline for one-hot encoding each categorical column\n",
        "for col_name in categorical_columns:\n",
        "    # Create a StringIndexer for the current column\n",
        "    indexer = StringIndexer(inputCol=col_name, outputCol=col_name + '_index')\n",
        "\n",
        "    # Create a OneHotEncoder for the indexed column\n",
        "    encoder = OneHotEncoder(inputCol=col_name + '_index', outputCol=col_name + '_encoded')\n",
        "\n",
        "    # Append the one-hot encoded column name to the list\n",
        "    one_hot_encoded_cols.append(col_name + '_encoded')\n",
        "\n",
        "    # Define the stages of the pipeline\n",
        "    stages = [indexer, encoder]\n",
        "\n",
        "    # Create the pipeline\n",
        "    pipeline = Pipeline(stages=stages)\n",
        "\n",
        "    # Fit and transform the train and test DataFrames\n",
        "    train = pipeline.fit(train).transform(train)\n",
        "    test = pipeline.fit(test).transform(test)\n",
        "\n",
        "# Print the shapes of the resulting DataFrames\n",
        "print('Training Features shape:', (train.count(), len(train.columns)))\n",
        "print('Testing Features shape:', (test.count(), len(test.columns)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKuTK08X8qH1"
      },
      "source": [
        "Align the training and test data (as the test data may not have the same columns in the encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpDqJ2tA8qH1",
        "outputId": "a256338d-12ce-4acc-cb8d-18c76ca3794b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Features shape: (245967, 26)\n",
            "Testing Features shape: (61545, 26)\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# List of common column names between train and test DataFrames\n",
        "common_columns = [col_name for col_name in train.columns if col_name in test.columns]\n",
        "\n",
        "# Select only the common columns in both train and test DataFrames\n",
        "train = train.select(*common_columns)\n",
        "test = test.select(*common_columns)\n",
        "\n",
        "# Print the shapes of the resulting DataFrames\n",
        "print('Training Features shape:', (train.count(), len(train.columns)))\n",
        "print('Testing Features shape:', (test.count(), len(test.columns)))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of columns to extract\n",
        "columns_extract = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n",
        "                  'DAYS_BIRTH', 'DAYS_EMPLOYED', 'NAME_EDUCATION_TYPE_index',\n",
        "                  'DAYS_ID_PUBLISH', 'CODE_GENDER_index', 'AMT_ANNUITY',\n",
        "                  'DAYS_REGISTRATION', 'AMT_GOODS_PRICE', 'AMT_CREDIT',\n",
        "                  'ORGANIZATION_TYPE_index', 'DAYS_LAST_PHONE_CHANGE',\n",
        "                  'NAME_INCOME_TYPE_index', 'AMT_INCOME_TOTAL', 'OWN_CAR_AGE', 'TARGET']\n",
        "\n",
        "# Select the specified columns from 'df_full'\n",
        "train = train.select(columns_extract)\n",
        "test = test.select(columns_extract)\n",
        "\n",
        "# Print the shapes of the resulting DataFrames\n",
        "print('Training Features shape:', (train.count(), len(train.columns)))\n",
        "print('Testing Features shape:', (test.count(), len(test.columns)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl-A6MpXqMLw",
        "outputId": "7e363a7c-9491-4567-81e0-b8d500f898d3"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Features shape: (245970, 18)\n",
            "Testing Features shape: (61549, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvHa_sIt8qH4"
      },
      "source": [
        "Fill in missing data and scale"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Imputer\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Feature names\n",
        "features = train.columns\n",
        "\n",
        "# Median imputation of missing values\n",
        "imputer = Imputer(inputCols=features, outputCols=features, strategy='median')\n",
        "imputer_model = imputer.fit(train)\n",
        "\n",
        "train = imputer_model.transform(train)\n",
        "test = imputer_model.transform(test)\n",
        "\n",
        "# Create a feature vector assembler\n",
        "vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "train = vector_assembler.transform(train)\n",
        "test = vector_assembler.transform(test)\n",
        "\n",
        "# Scale each feature to 0-1\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
        "scaler_model = scaler.fit(train)\n",
        "\n",
        "train = scaler_model.transform(train)\n",
        "test = scaler_model.transform(test)\n",
        "\n",
        "# Print data shapes\n",
        "print('Training data shape:', (train.count(), len(train.columns)))\n",
        "print('Testing data shape:', (test.count(), len(test.columns)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3ve1xRWeoP8",
        "outputId": "396b8395-72ab-4692-80a0-773ed768dd39"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (245968, 20)\n",
            "Testing data shape: (61547, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lf3dk_L8qH6"
      },
      "source": [
        "Fit random forest"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "random_forest = RandomForestClassifier(\n",
        "    labelCol='TARGET',\n",
        "    featuresCol='scaled_features',  # Replace with your feature column name\n",
        "    numTrees=100,\n",
        "    featureSubsetStrategy=\"auto\",\n",
        "    seed=50,\n",
        "    subsamplingRate=1.0,\n",
        "    maxDepth=5,\n",
        "    impurity=\"gini\",\n",
        "    minInstancesPerNode=1,\n",
        "    minInfoGain=0.0,\n",
        "    maxMemoryInMB=256,\n",
        "    cacheNodeIds=False,\n",
        "    checkpointInterval=10,\n",
        "    maxBins=32,\n",
        "    minWeightFractionPerNode=0.0,\n",
        ")\n",
        "\n",
        "# Create a pipeline for data preparation and training\n",
        "pipeline = Pipeline(stages=[random_forest])\n",
        "\n",
        "# Train the model on the training data\n",
        "model = pipeline.fit(train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.transform(test)\n",
        "\n",
        "# Extract feature importances\n",
        "feature_importances = model.stages[-1].featureImportances\n",
        "\n",
        "# Evaluate the model (You may need to adjust this depending on your evaluation metric)\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"TARGET\", rawPredictionCol=\"prediction\")\n",
        "area_under_curve = evaluator.evaluate(predictions)\n",
        "\n",
        "print(\"Area Under ROC:\", area_under_curve)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VcCT89w47vq",
        "outputId": "ab2645f7-8291-438e-c5c5-e7d3dce44d2c"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area Under ROC: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importances"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU8xR63YITwM",
        "outputId": "24f0e710-a0c0-4c50-e6ae-a2e8bc11f418"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SparseVector(18, {0: 0.0035, 1: 0.0101, 2: 0.012, 3: 0.0015, 4: 0.0003, 5: 0.0003, 6: 0.0, 7: 0.0004, 8: 0.0, 9: 0.0, 10: 0.0002, 11: 0.0, 13: 0.0004, 14: 0.0003, 15: 0.0, 16: 0.0, 17: 0.971})"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ass3",
      "language": "python",
      "name": "ass3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}